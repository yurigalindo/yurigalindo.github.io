---
layout: post
title:  "Understanding CLIP Robustness"
date:   2022-06-01 22:21:59 +00:00
image: /images/CLIP.png
categories: research
author: "Yuri Galindo"
venue: "The Art of Robustness: <strong>CVPR Workshop</strong>"
authors: "<strong>Leonid Keselman</strong>, FÃ¡bio A. Faria"
arxiv: https://artofrobust.github.io/short_paper/19.pdf
---

Tested hypotheses for the lack of robustness of neural networks with the CLIP model.

<blockquote>
  <p>
    Neural networks show a lack of robustness under adverse conditions, such as dealing with new datasets/distributions and adversarial perturbations. Some works in the literature, based on experiments with Resnet models trained on Imagenet, elect possible culprits such as the vulnerability to high frequency disturbances and dependence on non-robust features. Contrastive Language-Image Pre-training (CLIP) has been proposed as a new learning procedure which has improved robustness to new distributions but low robustness to adversarial examples. Therefore, CLIP presents an ideal opportunity for measuring how robust features and frequency sensitivity are associated with robustness to data shift. In this sense, we measure the vulnerability of CLIP model to high frequency perturbations, and perform image generation and inpainting tasks for assessment of robust features. In the performed experiments, the CLIP model is shown to be more robust to higher frequency perturbations and less robust to lower frequency perturbations, indicating a higher  dependence on features with lower frequency. Finally, the images generated by CLIP were of low quality, indicating a lack of robust features.

  </p>
</blockquote>
